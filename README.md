# Spark Assignment

# First Assignment

### Exercise 1: Data exploration using Spark
### Exercise 2: Spark streaming
### Exercise 3: More data exploration

In general, this assignment was about learning how to use Spark for big data
processing. It includes 3 exercises from which the 1 st and the 3 rd involved data
exploration using Spark and the 2 nd involved the implementation of a twitter
based stream processing application.
To solve this exercises I had to familiarize myself with the spark commands, the
concepts of RDDs and DStreams and with the Scala programming language.

# Second Assignment
The task of this assignment was to identify the actors linked to Kevin Bacon by
the six degrees of separation. Two lists were given, one with the actors and
another one with the actresses. Those lists also included all of the movies and
the TV series in which those actors/actresses have starred. For the purpose of
this assignment I took into account only the movies of this decade and none of
the TV series. It was very important to find an efficient way to process the data
because of their size.
The solution of this assignment was based on parsing the data and transforming
them to a graph. To solve this assignment I had to familiarize myself with GraphX
and action and transformation operations, to understand the importance of
cache and the way it operates, to learn how to use SparkListener and to
experiment with different solving methods to find the most efficient way to deal
with each sub-task.

# Third Assignment
This assignment was about replacing the regular GATK pipeline with a distributed
Spark version. It is divided in three parts:
Part 1: Chunking the input Fastq files
For this part we had to create the input for the parallelized GATK pipeline from
two FASTQ files (which are typically generated by a DNA sequencing
experiment). To solve this task we had to create interleaved chunks from the
data of fastq_l and fastq_r and for that purpose I had to familiarize with
partitioning the data and work on each partition. The next step of the GATK
pipeline performed in parallel on each of those chunks.
Part 2 : DNA Sequence Analysis
For the second part of this assignment, which was also the biggest part, we had
to perform the DNA sequence analysis which includes several steps the most
interesting of which, from the perspective of my implementation, was the load
balancing step. In this step we had to map the chromosome numbers to
chromosome regions in such a way that the regions will end up with similar
number of records. For the solution of this step I had to find a tactic on how to
choose which chromosome number will be mapped on which region.
Part 3 : Porting the solution to a cluster
For the third part of this assignment we had to implement a library so that we
are able to interact with HDFS. The code I wrote is in Scala and includes the
methods that I needed to
